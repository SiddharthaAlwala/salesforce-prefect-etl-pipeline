# Salesforce â†’ Polars â†’ JSON ETL Pipeline (Prefect)

## ğŸ“Œ Overview
This project implements an ETL pipeline using [Prefect](https://www.prefect.io/) that:
1. **Extracts** data from Salesforce via SOQL and saves it to CSV.
2. **Processes** the CSV using [Polars](https://pola.rs/) for data cleaning and aggregation.
3. **Loads** the processed CSV into a JSON file.
4. **Parallel QA/Sidecar** flow (schema checks, dedup, profiling, parquet snapshot, rowcount drift)
5. **Orchestrator** flow that extracts once and runs both Phase-1 (sequential ETL) and Phase-2 (parallel QA) together.

Designed for teams migrating or syncing Salesforce data with repeatable, observable pipelines.

---
## ğŸ“„ Description
A **config-driven** ETL for Salesforce data that scales across many objects. You define each objectâ€™s **fields**, **group_by**, and **metrics** once in a central registry, and the pipeline runs the same Extract â†’ Process â†’ Load sequence for every object. A **parallel QA** branch runs alongside ETL to produce quality artifacts (schema reports, profiles, parquet snapshots) and simple drift alerts.

**Intended audience**: data engineers, analytics engineers, and platform teams building reliable Salesforce data ingestion with lightweight orchestration.

## âœ¨ Features

**Object-aware ETL** driven by configs/salesforce_objects.py

**Phase-1 (Sequential)**: SOQL â†’ CSV â†’ Polars aggregation â†’ JSON

**Phase-2 (Parallel QA)**: schema/non-empty gates â†’ dedup/profile/parquet/drift

**Single Extract, Dual Branches**: orchestrator extracts once and fans out

**Robust JSON serialization**: handles datetimes, Polars/NumPy types

**Safe I/O**: timestamped raw filenames to avoid clobbering across runs

**Scheduling**: simple cron via flow.serve() every 15 minutes

**Extensible**: add custom Salesforce objects & validators easily



## ğŸ§© Project Structure
```bash
sf-prefect-pipeline/
â”œâ”€ configs/
â”‚  â””â”€ salesforce_objects.py           # Object registry: fields, required cols, group_by, metrics
â”œâ”€ utils/
â”‚  â””â”€ paths.py                        # Path builders: build_paths(), build_qc_paths()
â”œâ”€ tasks/
â”‚  â”œâ”€ extract.py                      # resolve_extract_plan(), extract_salesforce_to_csv (task)
â”‚  â”œâ”€ process.py                      # process_object_data (task, Polars aggregations)
â”‚  â”œâ”€ load.py                         # load_csv_to_json (task, robust JSON)
â”‚  â””â”€ quality_parallel.py             # QA tasks: schema/nonempty/dedup/profile/parquet/drift
â”œâ”€ flows/
â”‚  â”œâ”€ sf_csv_polars_json_flow.py      # Phase-1: sequential ETL flow
â”‚  â”œâ”€ object_parallel_flow.py         # Phase-2: parallel QA flow
â”‚  â””â”€ sf_etl_orchestrator_flow.py     # Orchestrator: runs Phase-1 + Phase-2 together
â”œâ”€ serve_orchestrator_15min.py        # Schedule orchestrator every 15 minutes
â”œâ”€ serve_phase1_15min.py              # (optional) Schedule Phase-1 only
â”œâ”€ serve_parallel_qa_15min.py         # (optional) Schedule Phase-2 only
â”œâ”€ run_multiple_times.py              # Quick local runner (typically calls orchestrator)
â”œâ”€ data/
â”‚  â”œâ”€ raw/                            # <object>_YYYYMMDD-HHMMSS_<RUNID>.csv
â”‚  â”œâ”€ processed/
â”‚  â”‚  â”œâ”€ <object>/summary.csv
â”‚  â”‚  â””â”€ qa/<object>/
â”‚  â”‚      â”œâ”€ dedup.csv
â”‚  â”‚      â”œâ”€ profile.json
â”‚  â”‚      â”œâ”€ schema_report.json
â”‚  â”‚      â”œâ”€ snapshot.parquet
â”‚  â”‚      â””â”€ rowcount.txt
â”‚  â””â”€ output/
â”‚     â””â”€ <object>/summary.json
â””â”€ README.md

```

---

## âš¡ Installation
```bash
1. **Create and activate local environment**

conda create -p .\.conda_env python=3.11 -y
conda activate .\.conda_env
```

Install dependencies
```bash
    python -m pip install --upgrade pip
    python -m pip install prefect polars simple-salesforce python-dotenv pyarrow pandas numpy
```

Add Salesforce credentials
    Create .env in project root:
```bash

        SF_USERNAME=your_username
        SF_PASSWORD=your_password
        SF_TOKEN=your_security_token
        SF_DOMAIN=login
```

â–¶ï¸ Running Manually


Run the flow 3 times for testing:
```bash
    python run_multiple_times.py
```

â° Scheduling Every 15 Minutes

    1. Start Prefect server
```bash
        python -m prefect server start
```
    2. Apply the deployment and start agent
```bash
        python deployments/apply_15min_deployment.py
        python -m prefect agent start -q default
```

    3. Open Prefect UI
        http://127.0.0.1:4200

## ğŸ§° Technologies

**Python 3.10+**
**Prefect 2** (flows, task runners, scheduling)

**Polars** (fast DataFrame engine)

**simple-salesforce** (Salesforce API)

**python-dotenv** (env management)

**PyArrow / Parquet** (columnar snapshots)

## âœ… Notes

You can pass a **custom SOQL** to the orchestrator/flows; processing still uses the registryâ€™s 
```bash
group_by/metrics.
```

Tasks include **retries** and structured **logging**.

Empty pulls are handled gracefully: headers-only CSV is written; QA will flag empty data.

Raw files are timestamped by default to avoid clobbering concurrent runs.

QA failures can be **advisory** (non-strict) or **strict** (fail_on_qa_error=True).